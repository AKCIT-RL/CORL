{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from mujoco_playground import registry\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from algorithms.utils.wrapper_gym import GymWrapper\n",
    "from algorithms.offline.any_percent_bc import get_actor_from_checkpoint as get_actor_from_checkpoint_bc, eval_actor as eval_actor_bc\n",
    "from algorithms.offline.td3_bc import get_actor_from_checkpoint as get_actor_from_checkpoint_td3_bc, eval_actor as eval_actor_td3_bc\n",
    "from algorithms.offline.sac_n import get_actor_from_checkpoint as get_actor_from_checkpoint_sac_n, eval_actor as eval_actor_sac_n\n",
    "get_actor_from_checkpoint = {\n",
    "    \"BC\": get_actor_from_checkpoint_bc,\n",
    "    \"TD3-BC\": get_actor_from_checkpoint_td3_bc,\n",
    "    \"SAC-N\": get_actor_from_checkpoint_sac_n,\n",
    "}\n",
    "eval_actor = {\n",
    "    \"BC\": eval_actor_bc,\n",
    "    \"TD3-BC\": eval_actor_td3_bc,\n",
    "    \"SAC-N\": eval_actor_sac_n,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export XLA_PYTHON_CLIENT_PREALLOCATE=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "def collect_checkpoint_info():\n",
    "    base_path = Path(\"../checkpoints\")\n",
    "    info_dict = []\n",
    "    \n",
    "    # Iterate through algorithm directories\n",
    "    for algo_dir in base_path.iterdir():\n",
    "        if algo_dir.is_dir():\n",
    "            # Iterate through experiment directories\n",
    "            for exp_dir in algo_dir.iterdir():\n",
    "                if exp_dir.is_dir():\n",
    "                    config_file = exp_dir / \"config.yaml\"\n",
    "                    if config_file.exists():\n",
    "                        with open(config_file, 'r') as f:\n",
    "                            config = yaml.safe_load(f)\n",
    "                            info_dict.append({\n",
    "                                'checkpoint_path': config.get('checkpoints_path'),\n",
    "                                'dataset_id': config.get('dataset_id'),\n",
    "                                'env': config.get('env'),\n",
    "                                'model': exp_dir.name.split(\"-\")[0],\n",
    "                                'difficulty': config.get('dataset_id').split(\"-\")[1]\n",
    "                            })\n",
    "    \n",
    "    return info_dict\n",
    "\n",
    "# Usage\n",
    "path_model = collect_checkpoint_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read existing results if file exists, otherwise create empty DataFrame\n",
    "try:\n",
    "    existing_results = pd.read_csv(\"results_offline.csv\")\n",
    "    existing_checkpoints = set(existing_results['checkpoint_path'].values)\n",
    "except FileNotFoundError:\n",
    "    existing_checkpoints = set()\n",
    "\n",
    "# Filter out paths that have already been evaluated\n",
    "path_model = [p for p in path_model if p[\"checkpoint_path\"] not in existing_checkpoints]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for p in path_model:\n",
    "    print(\"-\"*100)\n",
    "    print(f\"ENV: {p['env']}\")\n",
    "    print(\"-\"*100)\n",
    "    print()\n",
    "    checkpoint_path = os.path.join(\"..\", p[\"checkpoint_path\"])\n",
    "    with open(os.path.join(checkpoint_path, \"config.yaml\")) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    if p[\"model\"] == \"TD3\":\n",
    "        p[\"model\"] = \"TD3-BC\"\n",
    "    if p[\"model\"] == \"SAC\":\n",
    "        p[\"model\"] = \"SAC-N\"\n",
    "\n",
    "    env = registry.load(p[\"env\"])\n",
    "    env_cfg = registry.get_default_config(p[\"env\"])\n",
    "    randomizer = registry.get_domain_randomizer(p[\"env\"])\n",
    "\n",
    "    render_trajectory = []\n",
    "\n",
    "    def render_callback(_, state):\n",
    "        render_trajectory.append(state)\n",
    "\n",
    "    env_wrapped = GymWrapper(\n",
    "        env,\n",
    "        num_actors=1,\n",
    "        seed=1,\n",
    "        episode_length=env_cfg.episode_length,\n",
    "        action_repeat=1,\n",
    "        render_callback=render_callback,\n",
    "        randomization_fn=randomizer,\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "\n",
    "    state_dim = env_wrapped.observation_space.shape[0] \n",
    "    action_dim = env_wrapped.action_space.shape[0]\n",
    "    max_action = 1.0\n",
    "\n",
    "    # ------------- OFFLINE RL EVALUATION\n",
    "    actor = get_actor_from_checkpoint[p[\"model\"]](checkpoint_path=checkpoint_path, state_dim=state_dim, action_dim=action_dim, max_action=max_action)\n",
    "    episode_rewards = eval_actor[p[\"model\"]](actor=actor, env=env_wrapped, device=config[\"device\"], n_episodes=10, seed=0)\n",
    "    p[\"episode_rewards_mean\"] = episode_rewards.mean()\n",
    "    p[\"episode_rewards_std\"] = episode_rewards.std()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"results_offline.csv\")\n",
    "df_new = pd.DataFrame.from_dict(path_model)\n",
    "df_new = df_new.dropna(subset=[\"episode_rewards_mean\"])\n",
    "df = pd.concat([df, df_new], ignore_index=True)\n",
    "df.to_csv(\"results_offline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d3rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
