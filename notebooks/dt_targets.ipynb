{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minari\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "import minari\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from algorithms.utils.wrapper_gym import get_env\n",
    "from algorithms.utils.dataset import qlearning_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MINARI_DATASETS_PATH\"] = \"/home/luanagbmartins/Documents/CEIA/offline_to_online/CORL/datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/luanagbmartins/Documents/CEIA/offline_to_online/CORL/datasets\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print(os.getenv(\"MINARI_DATASETS_PATH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumsum(x: np.ndarray, gamma: float) -> np.ndarray:\n",
    "    cumsum = np.zeros_like(x)\n",
    "    cumsum[-1] = x[-1]\n",
    "    for t in reversed(range(x.shape[0] - 1)):\n",
    "        cumsum[t] = x[t] + gamma * cumsum[t + 1]\n",
    "    return cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = minari.load_dataset(\"playground/G1JoystickRoughTerrain-expert-v0\")\n",
    "\n",
    "traj, traj_len = [], []\n",
    "\n",
    "obs = []\n",
    "\n",
    "data_ = defaultdict(list)\n",
    "for episode in dataset.iterate_episodes():\n",
    "    data_[\"observations\"] = episode.observations\n",
    "    data_[\"actions\"] = episode.actions\n",
    "    data_[\"rewards\"] = episode.rewards\n",
    "    data_[\"terminals\"] = episode.terminations | episode.truncations\n",
    "\n",
    "    episode_data = {k: np.array(v, dtype=np.float32) for k, v in data_.items()}\n",
    "    # return-to-go if gamma=1.0, just discounted returns else\n",
    "    episode_data[\"returns\"] = discounted_cumsum(\n",
    "        episode_data[\"rewards\"], gamma=1.0\n",
    "    )\n",
    "    traj.append(episode_data)\n",
    "    traj_len.append(episode_data[\"actions\"].shape[0])\n",
    "    # reset trajectory buffer\n",
    "    data_ = defaultdict(list)\n",
    "\n",
    "    obs.append(episode.observations)\n",
    "\n",
    "# needed for normalization, weighted sampling, other stats can be added also\n",
    "info = {\n",
    "    \"obs_mean\": np.concatenate(obs).mean(0, keepdims=True),\n",
    "    \"obs_std\": np.concatenate(obs).std(0, keepdims=True) + 1e-6,\n",
    "    \"traj_lens\": np.array(traj_len),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearning_dataset(dataset: minari.MinariDataset) -> Dict[str, np.ndarray]:\n",
    "    obs, next_obs, actions, rewards, dones = [], [], [], [], []\n",
    "\n",
    "    for episode in dataset.iterate_episodes():\n",
    "        obs.append(episode.observations[:-1].astype(np.float32))\n",
    "        next_obs.append(episode.observations[1:].astype(np.float32))\n",
    "        actions.append(episode.actions.astype(np.float32))\n",
    "        rewards.append(episode.rewards)\n",
    "        dones.append(episode.terminations | episode.truncations)\n",
    "\n",
    "    return {\n",
    "        \"observations\": np.concatenate(obs),\n",
    "        \"actions\": np.concatenate(actions),\n",
    "        \"next_observations\": np.concatenate(next_obs),\n",
    "        \"rewards\": np.concatenate(rewards),\n",
    "        \"terminals\": np.concatenate(dones),\n",
    "    }\n",
    "\n",
    "\n",
    "qdataset = qlearning_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = qdataset[\"actions\"].flatten()\n",
    "print(f\"Min action value: {actions.min()}\")\n",
    "print(f\"Max action value: {actions.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_dict):\n",
    "        self.observations = dataset_dict[\"observations\"]\n",
    "        self.actions = dataset_dict[\"actions\"]\n",
    "        self.rewards = dataset_dict[\"rewards\"]\n",
    "        self.next_observations = dataset_dict[\"next_observations\"]\n",
    "        self.terminals = dataset_dict[\"terminals\"]\n",
    "        self.size = len(self.observations)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [\n",
    "            torch.from_numpy(self.observations[idx]),\n",
    "            torch.from_numpy(self.actions[idx]),\n",
    "            torch.tensor(self.rewards[idx], dtype=torch.float32),\n",
    "            torch.from_numpy(self.next_observations[idx]),\n",
    "            torch.tensor(self.terminals[idx], dtype=torch.float32),\n",
    "        ]\n",
    "\n",
    "\n",
    "# Create the replay buffer and dataloader\n",
    "replay_buffer = ReplayBuffer(qdataset)\n",
    "dataloader = DataLoader(replay_buffer, batch_size=128, shuffle=True)\n",
    "\n",
    "# Now you can iterate over the dataloader\n",
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch), batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script to analyze the Go1Footstand dataset and calculate appropriate target returns\n",
    "for Decision Transformer training.\n",
    "\"\"\"\n",
    "\n",
    "import minari\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_dataset(dataset_id):\n",
    "    \"\"\"Analyze dataset and calculate return statistics.\"\"\"\n",
    "    print(f\"Loading dataset: {dataset_id}\")\n",
    "    dataset = minari.load_dataset(dataset_id)\n",
    "    \n",
    "    returns = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in dataset.iterate_episodes():\n",
    "        # Calculate episode return\n",
    "        episode_return = sum(episode.rewards)\n",
    "        episode_length = len(episode.rewards)\n",
    "        \n",
    "        returns.append(episode_return)\n",
    "        episode_lengths.append(episode_length)\n",
    "    \n",
    "    returns = np.array(returns)\n",
    "    returns = np.nan_to_num(returns)\n",
    "    episode_lengths = np.array(episode_lengths)\n",
    "\n",
    "    print(f\"\\nDataset Statistics for {dataset_id}:\")\n",
    "    print(f\"Number of episodes: {len(returns)}\")\n",
    "    print(f\"Episode lengths - Mean: {episode_lengths.mean():.1f}, Std: {episode_lengths.std():.1f}\")\n",
    "    print(f\"Episode lengths - Min: {episode_lengths.min()}, Max: {episode_lengths.max()}\")\n",
    "    print(f\"\\nReturn Statistics:\")\n",
    "    print(f\"Mean return: {returns.mean():.1f}\")\n",
    "    print(f\"Std return: {returns.std():.1f}\")\n",
    "    print(f\"Min return: {returns.min():.1f}\")\n",
    "    print(f\"Max return: {returns.max():.1f}\")\n",
    "    print(f\"25th percentile: {np.percentile(returns, 25):.1f}\")\n",
    "    print(f\"50th percentile (median): {np.percentile(returns, 50):.1f}\")\n",
    "    print(f\"75th percentile: {np.percentile(returns, 75):.1f}\")\n",
    "    print(f\"90th percentile: {np.percentile(returns, 90):.1f}\")\n",
    "    print(f\"95th percentile: {np.percentile(returns, 95):.1f}\")\n",
    "    \n",
    "    # Calculate target returns for DT\n",
    "    # Typically use high percentile values as target returns\n",
    "    target_return_high = np.percentile(returns, 95)  # 95th percentile\n",
    "    target_return_medium = np.percentile(returns, 75)  # 75th percentile\n",
    "    \n",
    "    print(f\"\\nSuggested target_returns for DT:\")\n",
    "    print(f\"High target: {target_return_high:.0f}\")\n",
    "    print(f\"Medium target: {target_return_medium:.0f}\")\n",
    "    print(f\"Config format: target_returns: [{target_return_high:.0f}, {target_return_medium:.0f}]\")\n",
    "    \n",
    "    return target_return_high, target_return_medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: playground/H1InplaceGaitTracking-expert-v0\n",
      "\n",
      "Dataset Statistics for playground/H1InplaceGaitTracking-expert-v0:\n",
      "Number of episodes: 1100\n",
      "Episode lengths - Mean: 976.0, Std: 98.1\n",
      "Episode lengths - Min: 320, Max: 1000\n",
      "\n",
      "Return Statistics:\n",
      "Mean return: 34.6\n",
      "Std return: 4.8\n",
      "Min return: 11.5\n",
      "Max return: 38.5\n",
      "25th percentile: 32.9\n",
      "50th percentile (median): 36.4\n",
      "75th percentile: 37.5\n",
      "90th percentile: 38.0\n",
      "95th percentile: 38.3\n",
      "\n",
      "Suggested target_returns for DT:\n",
      "High target: 38\n",
      "Medium target: 37\n",
      "Config format: target_returns: [38, 37]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float32(38.337242), np.float32(37.462265))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = \"playground/H1InplaceGaitTracking-expert-v0\"\n",
    "analyze_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d3rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
