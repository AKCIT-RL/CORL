{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minari\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "import minari\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from algorithms.utils.wrapper_gym import get_env\n",
    "from algorithms.utils.dataset import qlearning_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MINARI_DATASETS_PATH\"] = \"/home/luanagbmartins/Documents/CEIA/offline_to_online/CORL/datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(dataset_id):\n",
    "    \"\"\"Analyze dataset and calculate return statistics.\"\"\"\n",
    "    print(f\"Loading dataset: {dataset_id}\")\n",
    "    dataset = minari.load_dataset(dataset_id)\n",
    "    \n",
    "    returns = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in dataset.iterate_episodes():\n",
    "        # Calculate episode return\n",
    "        episode_return = sum(episode.rewards)\n",
    "        episode_length = len(episode.rewards)\n",
    "        \n",
    "        returns.append(episode_return)\n",
    "        episode_lengths.append(episode_length)\n",
    "    \n",
    "    returns = np.array(returns)\n",
    "    print(returns)\n",
    "    # returns = np.nan_to_num(returns)\n",
    "    episode_lengths = np.array(episode_lengths)\n",
    "\n",
    "    print(f\"\\nDataset Statistics for {dataset_id}:\")\n",
    "    print(f\"Number of episodes: {len(returns)}\")\n",
    "    print(f\"Episode lengths - Mean: {episode_lengths.mean():.1f}, Std: {episode_lengths.std():.1f}\")\n",
    "    print(f\"Episode lengths - Min: {episode_lengths.min()}, Max: {episode_lengths.max()}\")\n",
    "    print(f\"\\nReturn Statistics:\")\n",
    "    print(f\"Mean return: {returns.mean():.1f}\")\n",
    "    print(f\"Std return: {returns.std():.1f}\")\n",
    "    print(f\"Min return: {returns.min():.1f}\")\n",
    "    print(f\"Max return: {returns.max():.1f}\")\n",
    "    print(f\"25th percentile: {np.percentile(returns, 25):.1f}\")\n",
    "    print(f\"50th percentile (median): {np.percentile(returns, 50):.1f}\")\n",
    "    print(f\"75th percentile: {np.percentile(returns, 75):.1f}\")\n",
    "    print(f\"90th percentile: {np.percentile(returns, 90):.1f}\")\n",
    "    print(f\"95th percentile: {np.percentile(returns, 95):.1f}\")\n",
    "    \n",
    "    # Calculate target returns for DT\n",
    "    # Typically use high percentile values as target returns\n",
    "    target_return_high = np.percentile(returns, 95)  # 95th percentile\n",
    "    target_return_medium = np.percentile(returns, 75)  # 75th percentile\n",
    "    \n",
    "    print(f\"\\nSuggested target_returns for DT:\")\n",
    "    print(f\"High target: {target_return_high:.0f}\")\n",
    "    print(f\"Medium target: {target_return_medium:.0f}\")\n",
    "    print(f\"Config format: target_returns: [{target_return_high:.0f}, {target_return_medium:.0f}]\")\n",
    "    \n",
    "    return target_return_high, target_return_medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"playground/G1JoystickRoughTerrain-expert-v0\"\n",
    "analyze_dataset(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d3rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
