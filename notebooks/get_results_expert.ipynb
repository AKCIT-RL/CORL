{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "import jax\n",
    "import numpy as np\n",
    "from etils import epath\n",
    "from tqdm import tqdm\n",
    "\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from mujoco_playground.config import locomotion_params\n",
    "\n",
    "from mujoco_playground import registry\n",
    "from mujoco_playground import wrapper, wrapper_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export XLA_PYTHON_CLIENT_PREALLOCATE=false "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = [\n",
    "    {'env': 'H1JoystickGaitTracking', \"model\": \"PPO\", \"checkpoint_path\": \"expert_checkpoints/H1JoystickGaitTracking\", \"policy_hidden_layer_sizes\": (256, 256)},\n",
    "    {'env': 'H1InplaceGaitTracking', \"model\": \"PPO\", \"checkpoint_path\": \"expert_checkpoints/H1InplaceGaitTracking\", \"policy_hidden_layer_sizes\": (256, 256)},\n",
    "    {'env': 'Go1JoystickRoughTerrain', \"model\": \"PPO\", \"checkpoint_path\": \"expert_checkpoints/Go1JoystickRoughTerrain\", \"policy_hidden_layer_sizes\": (256, 256)}, \n",
    "    {'env': 'Go1JoystickFlatTerrain', \"model\": \"PPO\", \"checkpoint_path\": \"expert_checkpoints/Go1JoystickFlatTerrain\", \"policy_hidden_layer_sizes\": (256, 256)},\n",
    "    {'env': 'Go1Handstand', \"model\": \"PPO\", \"checkpoint_path\": \"expert_checkpoints/Go1Handstand\", \"policy_hidden_layer_sizes\": (256, 256)}, \n",
    "    {'env': 'Go1Getup', \"model\": \"PPO\", \"checkpoint_path\": \"expert_checkpoints/Go1Getup\", \"policy_hidden_layer_sizes\": (256, 256)}, \n",
    "    {'env': 'Go1Footstand', \"model\": \"PPO\", \"checkpoint_path\": \"expert_checkpoints/Go1Footstand\", \"policy_hidden_layer_sizes\": (256, 256)},\n",
    "    {'env': 'G1JoystickRoughTerrain', \"model\": \"PPO\", \"checkpoint_path\": \"expert_checkpoints/G1JoystickRoughTerrain\", \"policy_hidden_layer_sizes\": (256, 256)}, \n",
    "    {'env': 'G1JoystickFlatTerrain', \"model\": \"PPO\", \"checkpoint_path\": \"expert_checkpoints/G1JoystickFlatTerrain\", \"policy_hidden_layer_sizes\": (256, 256)},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in path_model:\n",
    "    print(\"-\"*100)\n",
    "    print(f\"ENV: {p['env']}\")\n",
    "    print(\"-\"*100)\n",
    "    print()\n",
    "\n",
    "    env = registry.load(p[\"env\"])\n",
    "    env_cfg = registry.get_default_config(p[\"env\"])\n",
    "    randomizer = registry.get_domain_randomizer(p[\"env\"])\n",
    "\n",
    "\n",
    "    # ------------- EXPERT EVALUATION\n",
    "    ckpt_path = str(epath.Path(p[\"expert_checkpoint_path\"]).resolve())\n",
    "    FINETUNE_PATH = epath.Path(ckpt_path)\n",
    "    latest_ckpts = list(FINETUNE_PATH.glob(\"*\"))\n",
    "    latest_ckpts = [ckpt for ckpt in latest_ckpts if ckpt.is_dir()]\n",
    "    latest_ckpts.sort(key=lambda x: int(x.name))\n",
    "    latest_ckpt = latest_ckpts[-1]\n",
    "    restore_checkpoint_path = latest_ckpt\n",
    "\n",
    "    ppo_params = locomotion_params.brax_ppo_config(p[\"env\"])\n",
    "    ppo_training_params = dict(ppo_params)\n",
    "    ppo_training_params[\"num_timesteps\"] = 0\n",
    "    ppo_params[\"network_factory\"][\"policy_hidden_layer_sizes\"] = p[\"policy_hidden_layer_sizes\"]\n",
    "\n",
    "    network_factory = ppo_networks.make_ppo_networks\n",
    "    if \"network_factory\" in ppo_params:\n",
    "        del ppo_training_params[\"network_factory\"]\n",
    "        network_factory = functools.partial(\n",
    "            ppo_networks.make_ppo_networks, **ppo_params.network_factory\n",
    "        )\n",
    "\n",
    "    train_fn = functools.partial(\n",
    "        ppo.train,\n",
    "        **dict(ppo_training_params),\n",
    "        network_factory=network_factory,\n",
    "        randomization_fn=randomizer,\n",
    "    )\n",
    "\n",
    "    make_inference_fn, params, metrics = train_fn(\n",
    "        environment=registry.load(p[\"env\"]),\n",
    "        eval_env=registry.load(p[\"env\"]),\n",
    "        wrap_env_fn=wrapper.wrap_for_brax_training,\n",
    "        restore_checkpoint_path=restore_checkpoint_path,\n",
    "        seed=1,\n",
    "    )\n",
    "\n",
    "    jit_inference_fn = jax.jit(make_inference_fn(params, deterministic=True))\n",
    "    \n",
    "    def eval_expert(env, n_episodes, jit_inference_fn):\n",
    "        jit_reset = jax.jit(env.reset)\n",
    "        jit_step = jax.jit(env.step)\n",
    "        rng = jax.random.PRNGKey(12345)\n",
    "        rng, reset_rng = jax.random.split(rng)\n",
    "        episode_rewards = []\n",
    "        for _ in tqdm(range(n_episodes)):\n",
    "            state = jit_reset(reset_rng)\n",
    "            done = False\n",
    "            episode_reward = 0.0\n",
    "            for i in range(env_cfg.episode_length):\n",
    "                act_rng, rng = jax.random.split(rng)\n",
    "                action, _ = jit_inference_fn(state.obs, act_rng)\n",
    "                state = jit_step(state, action)\n",
    "                episode_reward += wrapper_torch._jax_to_torch(state.reward).cpu().numpy()\n",
    "                done = bool(wrapper_torch._jax_to_torch(state.done).cpu().numpy().item())\n",
    "                if done:\n",
    "                    break\n",
    "            episode_rewards.append(episode_reward)\n",
    "\n",
    "        return np.asarray(episode_rewards)\n",
    "    \n",
    "    episode_rewards = eval_expert(env, 10, jit_inference_fn, p[\"env\"])\n",
    "    p[\"episode_rewards_mean\"] = episode_rewards.mean()\n",
    "    p[\"episode_rewards_std\"] = episode_rewards.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(path_model).to_csv(\"results_expert.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d3rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
