{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "import jax\n",
    "import numpy as np\n",
    "from etils import epath\n",
    "from tqdm import tqdm\n",
    "\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from mujoco_playground.config import locomotion_params, manipulation_params\n",
    "\n",
    "from mujoco_playground import registry\n",
    "from mujoco_playground import wrapper, wrapper_torch\n",
    "\n",
    "import mediapy as media\n",
    "import mujoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export XLA_PYTHON_CLIENT_PREALLOCATE=false "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export MUJOCO_GL=egl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco.egl\n",
    "gl_context = mujoco.egl.GLContext(1024, 1024)\n",
    "gl_context.make_current()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = [\n",
    "    {'env': 'H1JoystickGaitTracking', \"model\": \"PPO\", \"checkpoint_path\": \"../expert_checkpoints/H1JoystickGaitTracking\"},\n",
    "    {'env': 'H1InplaceGaitTracking', \"model\": \"PPO\", \"checkpoint_path\": \"../expert_checkpoints/H1InplaceGaitTracking\"},\n",
    "    {'env': 'Go1JoystickRoughTerrain', \"model\": \"PPO\", \"checkpoint_path\": \"../expert_checkpoints/Go1JoystickRoughTerrain\"}, \n",
    "    {'env': 'Go1JoystickFlatTerrain', \"model\": \"PPO\", \"checkpoint_path\": \"../expert_checkpoints/Go1JoystickFlatTerrain\", \"policy_hidden_layer_sizes\": (1024, 512, 256)},\n",
    "    {'env': 'Go1Handstand', \"model\": \"PPO\", \"checkpoint_path\": \"../expert_checkpoints/Go1Handstand\"}, \n",
    "    {'env': 'Go1Getup', \"model\": \"PPO\", \"checkpoint_path\": \"../expert_checkpoints/Go1Getup\", \"policy_hidden_layer_sizes\": (512, 256, 128)}, \n",
    "    {'env': 'Go1Footstand', \"model\": \"PPO\", \"checkpoint_path\": \"../expert_checkpoints/Go1Footstand\"},\n",
    "    {'env': 'G1JoystickRoughTerrain', \"model\": \"PPO\", \"checkpoint_path\": \"../expert_checkpoints/G1JoystickRoughTerrain\"}, \n",
    "    {'env': 'G1JoystickFlatTerrain', \"model\": \"PPO\", \"checkpoint_path\": \"../expert_checkpoints/G1JoystickFlatTerrain\"},\n",
    "    {'env': 'PandaPickCube', \"model\": \"PPO\", \"checkpoint_path\": \"../expert_checkpoints/PandaPickCube\"}, \n",
    "    {'env': 'PandaOpenCabinet', \"model\": \"PPO\", \"checkpoint_path\": \"../expert_checkpoints/PandaOpenCabinet\"}, \n",
    "    {'env': 'Go2JoystickFlatTerrain', \"model\": \"PPO\", \"checkpoint_path\": \"logs/Go2JoystickFlatTerrain-20250725-143100/checkpoints\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in path_model:\n",
    "    print(\"-\"*100)\n",
    "    print(f\"ENV: {p['env']}\")\n",
    "    print(\"-\"*100)\n",
    "    print()\n",
    "\n",
    "    env = registry.load(p[\"env\"])\n",
    "    env_cfg = registry.get_default_config(p[\"env\"])\n",
    "    randomizer = registry.get_domain_randomizer(p[\"env\"])\n",
    "\n",
    "\n",
    "    # ------------- EXPERT EVALUATION\n",
    "    ckpt_path = str(epath.Path(p[\"checkpoint_path\"]).resolve())\n",
    "    FINETUNE_PATH = epath.Path(ckpt_path)\n",
    "    latest_ckpts = list(FINETUNE_PATH.glob(\"*\"))\n",
    "    latest_ckpts = [ckpt for ckpt in latest_ckpts if ckpt.is_dir()]\n",
    "    latest_ckpts.sort(key=lambda x: int(x.name))\n",
    "    latest_ckpt = latest_ckpts[-1]\n",
    "    restore_checkpoint_path = latest_ckpt\n",
    "\n",
    "    try:\n",
    "        ppo_params = locomotion_params.brax_ppo_config(p[\"env\"])\n",
    "    except:\n",
    "        ppo_params = manipulation_params.brax_ppo_config(p[\"env\"])\n",
    "\n",
    "    ppo_training_params = dict(ppo_params)\n",
    "    ppo_training_params[\"num_timesteps\"] = 0\n",
    "\n",
    "    if \"policy_hidden_layer_sizes\" in p:\n",
    "        ppo_params[\"network_factory\"][\"policy_hidden_layer_sizes\"] = p[\"policy_hidden_layer_sizes\"]\n",
    "\n",
    "    network_factory = ppo_networks.make_ppo_networks\n",
    "    if \"network_factory\" in ppo_params:\n",
    "        del ppo_training_params[\"network_factory\"]\n",
    "        network_factory = functools.partial(\n",
    "            ppo_networks.make_ppo_networks, **ppo_params.network_factory\n",
    "        )\n",
    "\n",
    "    train_fn = functools.partial(\n",
    "        ppo.train,\n",
    "        **dict(ppo_training_params),\n",
    "        network_factory=network_factory,\n",
    "        randomization_fn=randomizer,\n",
    "    )\n",
    "\n",
    "    make_inference_fn, params, metrics = train_fn(\n",
    "        environment=registry.load(p[\"env\"]),\n",
    "        eval_env=registry.load(p[\"env\"]),\n",
    "        wrap_env_fn=wrapper.wrap_for_brax_training,\n",
    "        restore_checkpoint_path=restore_checkpoint_path,\n",
    "        seed=1,\n",
    "    )\n",
    "\n",
    "    jit_inference_fn = jax.jit(make_inference_fn(params, deterministic=True))\n",
    "    \n",
    "    def eval_expert(env, n_episodes, jit_inference_fn):\n",
    "        jit_reset = jax.jit(env.reset)\n",
    "        jit_step = jax.jit(env.step)\n",
    "        rng = jax.random.PRNGKey(0)\n",
    "\n",
    "        rollout = []\n",
    "        episode_rewards = []\n",
    "        for _ in tqdm(range(n_episodes)):\n",
    "            rng, reset_rng = jax.random.split(rng)\n",
    "            state = jit_reset(reset_rng)\n",
    "            \n",
    "            rollout.append(state)\n",
    "            done = False\n",
    "            episode_reward = 0.0\n",
    "            for i in range(env_cfg.episode_length):\n",
    "                act_rng, rng = jax.random.split(rng)\n",
    "                action, _ = jit_inference_fn(state.obs, act_rng)\n",
    "                state = jit_step(state, action)\n",
    "                rollout.append(state)\n",
    "                episode_reward += wrapper_torch._jax_to_torch(state.reward).cpu().numpy()\n",
    "                done = bool(wrapper_torch._jax_to_torch(state.done).cpu().numpy().item())\n",
    "                if done:\n",
    "                    break\n",
    "            episode_rewards.append(episode_reward)\n",
    "\n",
    "        return np.asarray(episode_rewards), rollout\n",
    "    \n",
    "    episode_rewards, rollout = eval_expert(env, 20, jit_inference_fn)\n",
    "    p[\"episode_rewards_mean\"] = episode_rewards.mean()\n",
    "    p[\"episode_rewards_std\"] = episode_rewards.std()\n",
    "\n",
    "    render_every = 2\n",
    "    fps = 1.0 / env.dt / render_every\n",
    "    traj = rollout[::render_every]\n",
    "\n",
    "    scene_option = mujoco.MjvOption()\n",
    "    scene_option.geomgroup[2] = True\n",
    "    scene_option.geomgroup[3] = False\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_PERTFORCE] = True\n",
    "\n",
    "    try:\n",
    "        frames = env.render(\n",
    "            traj,\n",
    "            camera=\"track\",\n",
    "            scene_option=scene_option,\n",
    "            width=640,\n",
    "            height=480,\n",
    "        )\n",
    "    except:\n",
    "        render_every = 1\n",
    "        frames = env.render(rollout[::render_every])\n",
    "\n",
    "    media.write_video(f\"rollouts/expert_rollout-{p['env']}.mp4\", frames, fps=fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(path_model).to_csv(\"results_expert.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"rollouts/results_expert.csv\")\n",
    "df[[\"env\", \"episode_rewards_mean\", \"episode_rewards_std\"]].sort_values(by=\"env\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "offlinerl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
