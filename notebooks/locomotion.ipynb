{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpkYHwCqk7W-"
      },
      "source": [
        "![MuJoCo banner](https://raw.githubusercontent.com/google-deepmind/mujoco/main/banner.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBSdkbmGN2K-"
      },
      "source": [
        "### Copyright notice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UbO9uhtBSX5"
      },
      "source": [
        "> <p><small><small>Copyright 2025 DeepMind Technologies Limited.</small></p>\n",
        "> <p><small><small>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at <a href=\"http://www.apache.org/licenses/LICENSE-2.0\">http://www.apache.org/licenses/LICENSE-2.0</a>.</small></small></p>\n",
        "> <p><small><small>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</small></small></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNIJkb_FM2Ux"
      },
      "source": [
        "# Locomotion in The Playground! <a href=\"https://colab.research.google.com/github/google-deepmind/mujoco_playground/blob/main/learning/notebooks/locomotion.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" width=\"140\" align=\"center\"/></a>\n",
        "\n",
        "In this notebook, we'll walk through a few locomotion environments available in MuJoCo Playground.\n",
        "\n",
        "**A Colab runtime with GPU acceleration is required.** If you're using a CPU-only runtime, you can switch using the menu \"Runtime > Change runtime type\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "T5f4w3Kq2X14"
      },
      "outputs": [],
      "source": [
        "# @title Import packages for plotting and creating graphics\n",
        "import json\n",
        "import itertools\n",
        "import time\n",
        "from typing import Callable, List, NamedTuple, Optional, Union\n",
        "import numpy as np\n",
        "\n",
        "# Graphics and plotting.\n",
        "print(\"Installing mediapy:\")\n",
        "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "!pip install -q mediapy\n",
        "import mediapy as media\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# More legible printing from numpy.\n",
        "np.set_printoptions(precision=3, suppress=True, linewidth=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "ObF1UXrkb0Nd"
      },
      "outputs": [],
      "source": [
        "# @title Import MuJoCo, MJX, and Brax\n",
        "from datetime import datetime\n",
        "import functools\n",
        "import os\n",
        "from typing import Any, Dict, Sequence, Tuple, Union\n",
        "from brax import base\n",
        "from brax import envs\n",
        "from brax import math\n",
        "from brax.base import Base, Motion, Transform\n",
        "from brax.base import State as PipelineState\n",
        "from brax.envs.base import Env, PipelineEnv, State\n",
        "from brax.io import html, mjcf, model\n",
        "from brax.mjx.base import State as MjxState\n",
        "from brax.training.agents.ppo import networks as ppo_networks\n",
        "from brax.training.agents.ppo import train as ppo\n",
        "from brax.training.agents.sac import networks as sac_networks\n",
        "from brax.training.agents.sac import train as sac\n",
        "from etils import epath\n",
        "from flax import struct\n",
        "from flax.training import orbax_utils\n",
        "from IPython.display import HTML, clear_output\n",
        "import jax\n",
        "from jax import numpy as jp\n",
        "from matplotlib import pyplot as plt\n",
        "import mediapy as media\n",
        "from ml_collections import config_dict\n",
        "import mujoco\n",
        "from mujoco import mjx\n",
        "import numpy as np\n",
        "from orbax import checkpoint as ocp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "gYm2h7m8w3Nv"
      },
      "outputs": [],
      "source": [
        "# @title Import The Playground\n",
        "\n",
        "from mujoco_playground import wrapper\n",
        "from mujoco_playground import registry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcibXbyKt4FI"
      },
      "source": [
        "# Locomotion\n",
        "\n",
        "MuJoCo Playground contains a host of quadrupedal and bipedal environments (all listed below after running the command)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ox0Gze9Ct5AM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('BarkourJoystick',\n",
              " 'BerkeleyHumanoidJoystickFlatTerrain',\n",
              " 'BerkeleyHumanoidJoystickRoughTerrain',\n",
              " 'G1JoystickFlatTerrain',\n",
              " 'G1JoystickRoughTerrain',\n",
              " 'Go1JoystickRoughTerrain',\n",
              " 'Go1JoystickRoughTerrainEasy',\n",
              " 'Go1JoystickRoughTerrainMedium',\n",
              " 'Go1JoystickRoughTerrainHard',\n",
              " 'Go1JoystickFlatTerrain',\n",
              " 'Go1Getup',\n",
              " 'Go1Handstand',\n",
              " 'Go1Footstand',\n",
              " 'H1InplaceGaitTracking',\n",
              " 'H1JoystickGaitTracking',\n",
              " 'Op3Joystick',\n",
              " 'SpotFlatTerrainJoystick',\n",
              " 'SpotGetup',\n",
              " 'SpotJoystickGaitTracking',\n",
              " 'T1JoystickFlatTerrain',\n",
              " 'T1JoystickRoughTerrain')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "registry.locomotion.ALL_ENVS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kPJeoQeEJBSA"
      },
      "outputs": [],
      "source": [
        "env_name = \"Go1Getup\"\n",
        "env = registry.load(env_name)\n",
        "env_cfg = registry.get_default_config(env_name)\n",
        "# env_cfg.command_config.a = [2, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B9T_UVZYLDdM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "action_repeat: 1\n",
              "batch_size: 256\n",
              "discounting: 0.97\n",
              "entropy_cost: 0.01\n",
              "episode_length: 300\n",
              "learning_rate: 0.0003\n",
              "max_grad_norm: 1.0\n",
              "network_factory:\n",
              "  policy_hidden_layer_sizes: &id001 !!python/tuple\n",
              "  - 512\n",
              "  - 256\n",
              "  - 128\n",
              "  policy_obs_key: state\n",
              "  value_hidden_layer_sizes: *id001\n",
              "  value_obs_key: privileged_state\n",
              "normalize_observations: true\n",
              "num_envs: 8192\n",
              "num_evals: 5\n",
              "num_minibatches: 32\n",
              "num_timesteps: 50000000\n",
              "num_updates_per_batch: 4\n",
              "reward_scaling: 1.0\n",
              "unroll_length: 20"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from mujoco_playground.config import locomotion_params\n",
        "\n",
        "ppo_params = locomotion_params.brax_ppo_config(env_name)\n",
        "ppo_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aefr2OS01D9g"
      },
      "source": [
        "Domain randomization was used to make the policy robust to sim-to-real transfer. Certain environments in the Playground have domain randomization functions implemented. They're available in the registry and can be passed directly to brax RL algorithms. The [domain randomization](https://github.com/google-deepmind/mujoco_playground/blob/main/mujoco_playground/_src/locomotion/go1/randomize.py) function randomizes over friction, armature, center of mass of the torso, and link masses, amongst other simulation parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "ppo_params[\"episode_length\"] = 2000\n",
        "ppo_params[\"learning_rate\"] = 1e-4\n",
        "ppo_params[\"entropy_cost\"] = 0.02\n",
        "ppo_params[\"max_grad_norm\"] = 2.0\n",
        "ppo_params[\"num_evals\"] = 20\n",
        "ppo_params[\"num_minibatches\"] = 64\n",
        "ppo_params[\"num_updates_per_batch\"] = 8\n",
        "ppo_params[\"unroll_length\"] = 40\n",
        "ppo_params[\"reward_scaling\"] = 2.0\n",
        "ppo_params[\"num_envs\"] = 16384"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UVA4Bn681DZT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function mujoco_playground._src.locomotion.go1.randomize.domain_randomize(model: mujoco.mjx._src.types.Model, rng: jax.Array)>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "registry.get_domain_randomizer(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBEEQyY6M5OC"
      },
      "source": [
        "### Train\n",
        "\n",
        "The policy takes 7 minutes to train on an RTX 4090."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XKFzyP7wM5OD"
      },
      "outputs": [],
      "source": [
        "x_data, y_data, y_dataerr = [], [], []\n",
        "times = [datetime.now()]\n",
        "\n",
        "\n",
        "def progress(num_steps, metrics):\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    times.append(datetime.now())\n",
        "    x_data.append(num_steps)\n",
        "    y_data.append(metrics[\"eval/episode_reward\"])\n",
        "    y_dataerr.append(metrics[\"eval/episode_reward_std\"])\n",
        "\n",
        "    plt.xlim([0, ppo_params[\"num_timesteps\"] * 1.25])\n",
        "    plt.xlabel(\"# environment steps\")\n",
        "    plt.ylabel(\"reward per episode\")\n",
        "    plt.title(f\"y={y_data[-1]:.3f}\")\n",
        "    plt.errorbar(x_data, y_data, yerr=y_dataerr, color=\"blue\")\n",
        "\n",
        "    display(plt.gcf())\n",
        "\n",
        "\n",
        "randomizer = registry.get_domain_randomizer(env_name)\n",
        "ppo_training_params = dict(ppo_params)\n",
        "network_factory = ppo_networks.make_ppo_networks\n",
        "if \"network_factory\" in ppo_params:\n",
        "    del ppo_training_params[\"network_factory\"]\n",
        "    network_factory = functools.partial(\n",
        "        ppo_networks.make_ppo_networks, **ppo_params.network_factory\n",
        "    )\n",
        "\n",
        "train_fn = functools.partial(\n",
        "    ppo.train,\n",
        "    **dict(ppo_training_params),\n",
        "    network_factory=network_factory,\n",
        "    randomization_fn=randomizer,\n",
        "    progress_fn=progress,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FGrlulWbM5OD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0506 12:09:07.819140  581725 pjrt_stream_executor_client.cc:3077] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Failed to get module function: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
          ]
        },
        {
          "ename": "XlaRuntimeError",
          "evalue": "RESOURCE_EXHAUSTED: Failed to get module function: CUDA_ERROR_OUT_OF_MEMORY: out of memory",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m make_inference_fn, params, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_cfg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrap_env_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_for_brax_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime to jit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtimes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime to train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtimes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/d3rl/lib/python3.10/site-packages/brax/training/agents/ppo/train.py:584\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(environment, num_timesteps, episode_length, wrap_env, wrap_env_fn, action_repeat, num_envs, max_devices_per_host, num_eval_envs, learning_rate, entropy_cost, discounting, seed, unroll_length, batch_size, num_minibatches, num_updates_per_batch, num_evals, num_resets_per_eval, normalize_observations, reward_scaling, clipping_epsilon, gae_lambda, deterministic_eval, network_factory, progress_fn, normalize_advantage, eval_env, policy_params_fn, randomization_fn, restore_checkpoint_path, max_grad_norm, madrona_backend, augment_pixels)\u001b[0m\n\u001b[1;32m    582\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_evals \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 584\u001b[0m   metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_unpmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtraining_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalizer_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtraining_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtraining_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtraining_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m   logging\u001b[38;5;241m.\u001b[39minfo(metrics)\n\u001b[1;32m    593\u001b[0m   progress_fn(\u001b[38;5;241m0\u001b[39m, metrics)\n",
            "File \u001b[0;32m~/miniconda3/envs/d3rl/lib/python3.10/site-packages/brax/training/acting.py:134\u001b[0m, in \u001b[0;36mEvaluator.run_evaluation\u001b[0;34m(self, policy_params, training_metrics, aggregate_episodes)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key, unroll_key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key)\n\u001b[1;32m    133\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 134\u001b[0m eval_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_eval_unroll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munroll_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m eval_metrics \u001b[38;5;241m=\u001b[39m eval_state\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_metrics\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    136\u001b[0m eval_metrics\u001b[38;5;241m.\u001b[39mactive_episodes\u001b[38;5;241m.\u001b[39mblock_until_ready()\n",
            "    \u001b[0;31m[... skipping hidden 5 frame]\u001b[0m\n",
            "File \u001b[0;32m~/miniconda3/envs/d3rl/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:1297\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1295\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1297\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1300\u001b[0m   out_arrays \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_into_single_device_arrays()\n",
            "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Failed to get module function: CUDA_ERROR_OUT_OF_MEMORY: out of memory"
          ]
        }
      ],
      "source": [
        "make_inference_fn, params, metrics = train_fn(\n",
        "    environment=env,\n",
        "    eval_env=registry.load(env_name, config=env_cfg),\n",
        "    wrap_env_fn=wrapper.wrap_for_brax_training,\n",
        ")\n",
        "print(f\"time to jit: {times[1] - times[0]}\")\n",
        "print(f\"time to train: {times[-1] - times[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUxSNhq3UqmC"
      },
      "source": [
        "Let's rollout and render the resulting policy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBM89g5A2Yoi"
      },
      "outputs": [],
      "source": [
        "# Enable perturbation in the eval env.\n",
        "env_cfg = registry.get_default_config(env_name)\n",
        "eval_env = registry.load(env_name, config=env_cfg)\n",
        "jit_reset = jax.jit(eval_env.reset)\n",
        "jit_step = jax.jit(eval_env.step)\n",
        "jit_inference_fn = jax.jit(make_inference_fn(params, deterministic=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "C_1CY9xDoUKw"
      },
      "outputs": [],
      "source": [
        "# @title Rollout and Render\n",
        "from mujoco_playground._src.gait import draw_joystick_command\n",
        "\n",
        "x_vel = 1.0  # @param {type: \"number\"}\n",
        "y_vel = 0.0  # @param {type: \"number\"}\n",
        "yaw_vel = 0.0  # @param {type: \"number\"}\n",
        "\n",
        "\n",
        "def sample_pert(rng):\n",
        "    rng, key1, key2 = jax.random.split(rng, 3)\n",
        "    pert_mag = jax.random.uniform(\n",
        "        key1, minval=velocity_kick_range[0], maxval=velocity_kick_range[1]\n",
        "    )\n",
        "    duration_seconds = jax.random.uniform(\n",
        "        key2, minval=kick_duration_range[0], maxval=kick_duration_range[1]\n",
        "    )\n",
        "    duration_steps = jp.round(duration_seconds / eval_env.dt).astype(jp.int32)\n",
        "    state.info[\"pert_mag\"] = pert_mag\n",
        "    state.info[\"pert_duration\"] = duration_steps\n",
        "    state.info[\"pert_duration_seconds\"] = duration_seconds\n",
        "    return rng\n",
        "\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "rollout = []\n",
        "modify_scene_fns = []\n",
        "\n",
        "swing_peak = []\n",
        "rewards = []\n",
        "linvel = []\n",
        "angvel = []\n",
        "track = []\n",
        "foot_vel = []\n",
        "rews = []\n",
        "contact = []\n",
        "command = jp.array([x_vel, y_vel, yaw_vel])\n",
        "\n",
        "state = jit_reset(rng)\n",
        "if state.info[\"steps_since_last_pert\"] < state.info[\"steps_until_next_pert\"]:\n",
        "    rng = sample_pert(rng)\n",
        "state.info[\"command\"] = command\n",
        "for i in range(env_cfg.episode_length):\n",
        "    if state.info[\"steps_since_last_pert\"] < state.info[\"steps_until_next_pert\"]:\n",
        "        rng = sample_pert(rng)\n",
        "    act_rng, rng = jax.random.split(rng)\n",
        "    ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
        "    state = jit_step(state, ctrl)\n",
        "    state.info[\"command\"] = command\n",
        "    rews.append({k: v for k, v in state.metrics.items() if k.startswith(\"reward/\")})\n",
        "    rollout.append(state)\n",
        "    swing_peak.append(state.info[\"swing_peak\"])\n",
        "    rewards.append(\n",
        "        {k[7:]: v for k, v in state.metrics.items() if k.startswith(\"reward/\")}\n",
        "    )\n",
        "    linvel.append(env.get_global_linvel(state.data))\n",
        "    angvel.append(env.get_gyro(state.data))\n",
        "    track.append(\n",
        "        env._reward_tracking_lin_vel(\n",
        "            state.info[\"command\"], env.get_local_linvel(state.data)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    feet_vel = state.data.sensordata[env._foot_linvel_sensor_adr]\n",
        "    vel_xy = feet_vel[..., :2]\n",
        "    vel_norm = jp.sqrt(jp.linalg.norm(vel_xy, axis=-1))\n",
        "    foot_vel.append(vel_norm)\n",
        "\n",
        "    contact.append(state.info[\"last_contact\"])\n",
        "\n",
        "    xyz = np.array(state.data.xpos[env._torso_body_id])\n",
        "    xyz += np.array([0, 0, 0.2])\n",
        "    x_axis = state.data.xmat[env._torso_body_id, 0]\n",
        "    yaw = -np.arctan2(x_axis[1], x_axis[0])\n",
        "    modify_scene_fns.append(\n",
        "        functools.partial(\n",
        "            draw_joystick_command,\n",
        "            cmd=state.info[\"command\"],\n",
        "            xyz=xyz,\n",
        "            theta=yaw,\n",
        "            scl=abs(state.info[\"command\"][0]) / env_cfg.command_config.a[0],\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "render_every = 2\n",
        "fps = 1.0 / eval_env.dt / render_every\n",
        "traj = rollout[::render_every]\n",
        "mod_fns = modify_scene_fns[::render_every]\n",
        "\n",
        "scene_option = mujoco.MjvOption()\n",
        "scene_option.geomgroup[2] = True\n",
        "scene_option.geomgroup[3] = False\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_PERTFORCE] = True\n",
        "\n",
        "frames = eval_env.render(\n",
        "    traj,\n",
        "    camera=\"track\",\n",
        "    scene_option=scene_option,\n",
        "    width=640,\n",
        "    height=480,\n",
        "    modify_scene_fns=mod_fns,\n",
        ")\n",
        "media.show_video(frames, fps=fps, loop=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QHdoJ2r30En"
      },
      "source": [
        "Let's visualize the feet positions and the positional drift compared to the commanded linear and angular velocity. This is useful for debugging how well the policy follows the commands!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gyyynm3ozEet"
      },
      "outputs": [],
      "source": [
        "# @title Plot each foot in a 2x2 grid.\n",
        "\n",
        "swing_peak = jp.array(swing_peak)\n",
        "names = [\"FR\", \"FL\", \"RR\", \"RL\"]\n",
        "colors = [\"r\", \"g\", \"b\", \"y\"]\n",
        "fig, axs = plt.subplots(2, 2)\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    ax.plot(swing_peak[:, i], color=colors[i])\n",
        "    ax.set_ylim([0, env_cfg.reward_config.max_foot_height * 1.25])\n",
        "    ax.axhline(env_cfg.reward_config.max_foot_height, color=\"k\", linestyle=\"--\")\n",
        "    ax.set_title(names[i])\n",
        "    ax.set_xlabel(\"time\")\n",
        "    ax.set_ylabel(\"height\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "linvel_x = jp.array(linvel)[:, 0]\n",
        "linvel_y = jp.array(linvel)[:, 1]\n",
        "angvel_yaw = jp.array(angvel)[:, 2]\n",
        "\n",
        "# Plot whether velocity is within the command range.\n",
        "linvel_x = jp.convolve(linvel_x, jp.ones(10) / 10, mode=\"same\")\n",
        "linvel_y = jp.convolve(linvel_y, jp.ones(10) / 10, mode=\"same\")\n",
        "angvel_yaw = jp.convolve(angvel_yaw, jp.ones(10) / 10, mode=\"same\")\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(10, 10))\n",
        "axes[0].plot(linvel_x)\n",
        "axes[1].plot(linvel_y)\n",
        "axes[2].plot(angvel_yaw)\n",
        "\n",
        "axes[0].set_ylim(-env_cfg.command_config.a[0], env_cfg.command_config.a[0])\n",
        "axes[1].set_ylim(-env_cfg.command_config.a[1], env_cfg.command_config.a[1])\n",
        "axes[2].set_ylim(-env_cfg.command_config.a[2], env_cfg.command_config.a[2])\n",
        "\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.axhline(state.info[\"command\"][i], color=\"red\", linestyle=\"--\")\n",
        "\n",
        "labels = [\"dx\", \"dy\", \"dyaw\"]\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.set_ylabel(labels[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RHZvXgmzrEJ"
      },
      "source": [
        "## Handstand\n",
        "\n",
        "Additional policies are available for the Unitree Go1 such as fall-recovery, handstand, and footstand policies. We'll use the handstand policy as an opportunity to demonstrate finetuning policies from prior checkpoints. This will allow us to quickly iterate on training curriculums by modifying the enviornment config between runs.\n",
        "\n",
        "For the Go1 handstand policy, we'll first train with the default configuration, and then add an energy penalty to make the policy smoother and more likely to transfer onto the robot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYriZOAxzEk_"
      },
      "outputs": [],
      "source": [
        "from mujoco_playground.config import locomotion_params\n",
        "\n",
        "env_name = \"Go1Handstand\"\n",
        "env = registry.load(env_name)\n",
        "env_cfg = registry.get_default_config(env_name)\n",
        "ppo_params = locomotion_params.brax_ppo_config(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nB5ugbdS5kk"
      },
      "source": [
        "Let's create a checkpoint directory and then train a policy with checkpointing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyEDpHisS7eO"
      },
      "outputs": [],
      "source": [
        "ckpt_path = epath.Path(\"checkpoints\").resolve() / env_name\n",
        "ckpt_path.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"{ckpt_path}\")\n",
        "\n",
        "with open(ckpt_path / \"config.json\", \"w\") as fp:\n",
        "    json.dump(env_cfg.to_dict(), fp, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCRUYofXSNGT"
      },
      "outputs": [],
      "source": [
        "# @title Training fn definition\n",
        "x_data, y_data, y_dataerr = [], [], []\n",
        "times = [datetime.now()]\n",
        "\n",
        "\n",
        "def policy_params_fn(current_step, make_policy, params):\n",
        "    del make_policy  # Unused.\n",
        "    orbax_checkpointer = ocp.PyTreeCheckpointer()\n",
        "    save_args = orbax_utils.save_args_from_target(params)\n",
        "    path = ckpt_path / f\"{current_step}\"\n",
        "    orbax_checkpointer.save(path, params, force=True, save_args=save_args)\n",
        "\n",
        "\n",
        "def progress(num_steps, metrics):\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    times.append(datetime.now())\n",
        "    x_data.append(num_steps)\n",
        "    y_data.append(metrics[\"eval/episode_reward\"])\n",
        "    y_dataerr.append(metrics[\"eval/episode_reward_std\"])\n",
        "\n",
        "    plt.xlim([0, ppo_params[\"num_timesteps\"] * 1.25])\n",
        "    plt.xlabel(\"# environment steps\")\n",
        "    plt.ylabel(\"reward per episode\")\n",
        "    plt.title(f\"y={y_data[-1]:.3f}\")\n",
        "    plt.errorbar(x_data, y_data, yerr=y_dataerr, color=\"blue\")\n",
        "\n",
        "    display(plt.gcf())\n",
        "\n",
        "\n",
        "randomizer = registry.get_domain_randomizer(env_name)\n",
        "ppo_training_params = dict(ppo_params)\n",
        "network_factory = ppo_networks.make_ppo_networks\n",
        "if \"network_factory\" in ppo_params:\n",
        "    del ppo_training_params[\"network_factory\"]\n",
        "    network_factory = functools.partial(\n",
        "        ppo_networks.make_ppo_networks, **ppo_params.network_factory\n",
        "    )\n",
        "\n",
        "train_fn = functools.partial(\n",
        "    ppo.train,\n",
        "    **dict(ppo_training_params),\n",
        "    network_factory=network_factory,\n",
        "    randomization_fn=randomizer,\n",
        "    progress_fn=progress,\n",
        "    policy_params_fn=policy_params_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1oK80x1anPp"
      },
      "source": [
        "The initial policy takes 8 minutes to train on an RTX 4090."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY6P3abhSNGU"
      },
      "outputs": [],
      "source": [
        "make_inference_fn, params, metrics = train_fn(\n",
        "    environment=registry.load(env_name, config=env_cfg),\n",
        "    eval_env=registry.load(env_name, config=env_cfg),\n",
        "    wrap_env_fn=wrapper.wrap_for_brax_training,\n",
        ")\n",
        "print(f\"time to jit: {times[1] - times[0]}\")\n",
        "print(f\"time to train: {times[-1] - times[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s6PkZ4GWV4Z"
      },
      "source": [
        "Let's visualize the current policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WiWOtc_6WbcX"
      },
      "outputs": [],
      "source": [
        "# @title Rollout and Render\n",
        "inference_fn = make_inference_fn(params, deterministic=True)\n",
        "jit_inference_fn = jax.jit(inference_fn)\n",
        "\n",
        "eval_env = registry.load(env_name, config=env_cfg)\n",
        "jit_reset = jax.jit(eval_env.reset)\n",
        "jit_step = jax.jit(eval_env.step)\n",
        "\n",
        "rng = jax.random.PRNGKey(12345)\n",
        "rollout = []\n",
        "rewards = []\n",
        "torso_height = []\n",
        "actions = []\n",
        "torques = []\n",
        "power = []\n",
        "qfrc_constraint = []\n",
        "qvels = []\n",
        "power1 = []\n",
        "power2 = []\n",
        "for _ in range(10):\n",
        "    rng, reset_rng = jax.random.split(rng)\n",
        "    state = jit_reset(reset_rng)\n",
        "    for i in range(env_cfg.episode_length // 2):\n",
        "        act_rng, rng = jax.random.split(rng)\n",
        "        ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
        "        actions.append(ctrl)\n",
        "        state = jit_step(state, ctrl)\n",
        "        rollout.append(state)\n",
        "        rewards.append(\n",
        "            {k[7:]: v for k, v in state.metrics.items() if k.startswith(\"reward/\")}\n",
        "        )\n",
        "        torso_height.append(state.data.qpos[2])\n",
        "        torques.append(state.data.actuator_force)\n",
        "        qvel = state.data.qvel[6:]\n",
        "        power.append(jp.sum(jp.abs(qvel * state.data.actuator_force)))\n",
        "        qfrc_constraint.append(jp.linalg.norm(state.data.qfrc_constraint[6:]))\n",
        "        qvels.append(jp.max(jp.abs(qvel)))\n",
        "        frc = state.data.actuator_force\n",
        "        qvel = state.data.qvel[6:]\n",
        "        power1.append(jp.sum(frc * qvel))\n",
        "        power2.append(jp.sum(jp.abs(frc * qvel)))\n",
        "\n",
        "\n",
        "render_every = 2\n",
        "fps = 1.0 / eval_env.dt / render_every\n",
        "traj = rollout[::render_every]\n",
        "\n",
        "scene_option = mujoco.MjvOption()\n",
        "scene_option.geomgroup[2] = True\n",
        "scene_option.geomgroup[3] = False\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTFORCE] = False\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
        "\n",
        "frames = eval_env.render(\n",
        "    traj, camera=\"side\", scene_option=scene_option, height=480, width=640\n",
        ")\n",
        "media.show_video(frames, fps=fps, loop=False)\n",
        "\n",
        "power = jp.array(power1)\n",
        "print(f\"Max power: {jp.max(power)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5p0Z3PPSRik"
      },
      "source": [
        "Notice that the above policy looks jittery and unlikely to transfer on the robot. The max power output is also quite high.\n",
        "\n",
        "The sim-to-real deployment of the handstand policy was trained using a curriculum on the `energy_termination_threshold`, `energy` and `dof_acc`, which are config values that penalize high torques and high power output. Let's finetune the above policy with a decreased  `energy_termination_threshold`, as well as non-zero values for `energy` and `dof_acc` rewards to get a smoother policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrjoVL-_WN-r"
      },
      "source": [
        "### Finetune the previous checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTxAySRSSu96"
      },
      "outputs": [],
      "source": [
        "env_cfg = registry.get_default_config(env_name)\n",
        "env_cfg.energy_termination_threshold = 400  # lower energy termination threshold\n",
        "env_cfg.reward_config.energy = -0.003  # non-zero negative `energy` reward\n",
        "env_cfg.reward_config.dof_acc = -2.5e-7  # non-zero negative `dof_acc` reward\n",
        "\n",
        "FINETUNE_PATH = epath.Path(ckpt_path)\n",
        "latest_ckpts = list(FINETUNE_PATH.glob(\"*\"))\n",
        "latest_ckpts = [ckpt for ckpt in latest_ckpts if ckpt.is_dir()]\n",
        "latest_ckpts.sort(key=lambda x: int(x.name))\n",
        "latest_ckpt = latest_ckpts[-1]\n",
        "restore_checkpoint_path = latest_ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_M5IqOR6z4bV"
      },
      "outputs": [],
      "source": [
        "x_data, y_data, y_dataerr = [], [], []\n",
        "times = [datetime.now()]\n",
        "\n",
        "make_inference_fn, params, metrics = train_fn(\n",
        "    environment=registry.load(env_name, config=env_cfg),\n",
        "    eval_env=registry.load(env_name, config=env_cfg),\n",
        "    wrap_env_fn=wrapper.wrap_for_brax_training,\n",
        "    restore_checkpoint_path=restore_checkpoint_path,  # restore from the checkpoint!\n",
        "    seed=1,\n",
        ")\n",
        "print(f\"time to jit: {times[1] - times[0]}\")\n",
        "print(f\"time to train: {times[-1] - times[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tzG8eY2lz4dk"
      },
      "outputs": [],
      "source": [
        "# @title Rollout and Render Finetune Policy\n",
        "inference_fn = make_inference_fn(params, deterministic=True)\n",
        "jit_inference_fn = jax.jit(inference_fn)\n",
        "\n",
        "eval_env = registry.load(env_name, config=env_cfg)\n",
        "jit_reset = jax.jit(eval_env.reset)\n",
        "jit_step = jax.jit(eval_env.step)\n",
        "\n",
        "rng = jax.random.PRNGKey(12345)\n",
        "rollout = []\n",
        "rewards = []\n",
        "torso_height = []\n",
        "actions = []\n",
        "torques = []\n",
        "power = []\n",
        "qfrc_constraint = []\n",
        "qvels = []\n",
        "power1 = []\n",
        "power2 = []\n",
        "for _ in range(10):\n",
        "    rng, reset_rng = jax.random.split(rng)\n",
        "    state = jit_reset(reset_rng)\n",
        "    for i in range(env_cfg.episode_length // 2):\n",
        "        act_rng, rng = jax.random.split(rng)\n",
        "        ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
        "        actions.append(ctrl)\n",
        "        state = jit_step(state, ctrl)\n",
        "        rollout.append(state)\n",
        "        rewards.append(\n",
        "            {k[7:]: v for k, v in state.metrics.items() if k.startswith(\"reward/\")}\n",
        "        )\n",
        "        torso_height.append(state.data.qpos[2])\n",
        "        torques.append(state.data.actuator_force)\n",
        "        qvel = state.data.qvel[6:]\n",
        "        power.append(jp.sum(jp.abs(qvel * state.data.actuator_force)))\n",
        "        qfrc_constraint.append(jp.linalg.norm(state.data.qfrc_constraint[6:]))\n",
        "        qvels.append(jp.max(jp.abs(qvel)))\n",
        "        frc = state.data.actuator_force\n",
        "        qvel = state.data.qvel[6:]\n",
        "        power1.append(jp.sum(frc * qvel))\n",
        "        power2.append(jp.sum(jp.abs(frc * qvel)))\n",
        "\n",
        "\n",
        "render_every = 2\n",
        "fps = 1.0 / eval_env.dt / render_every\n",
        "traj = rollout[::render_every]\n",
        "\n",
        "scene_option = mujoco.MjvOption()\n",
        "scene_option.geomgroup[2] = True\n",
        "scene_option.geomgroup[3] = False\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTFORCE] = False\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
        "\n",
        "frames = eval_env.render(\n",
        "    traj, camera=\"side\", scene_option=scene_option, height=480, width=640\n",
        ")\n",
        "media.show_video(frames, fps=fps, loop=False)\n",
        "\n",
        "power = jp.array(power1)\n",
        "print(f\"Max power: {jp.max(power)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCyibqGMiAca"
      },
      "source": [
        "The final policy should exhibit smoother behavior and have less power output! Feel free to finetune the policy some more using different reward terms to get the best behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26o77FfWXvVp"
      },
      "source": [
        "# Bipedal\n",
        "\n",
        "MuJoCo Playground also comes with a host of bipedal environments, such as the Berkely Humanoid and the Unitree G1/H1. Let's demonstrate a joystick policy on the Berkeley Humanoid. The initial policy takes 17 minutes to train on an RTX 4090."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESNd18FUanPt"
      },
      "outputs": [],
      "source": [
        "env_name = \"BerkeleyHumanoidJoystickFlatTerrain\"\n",
        "env = registry.load(env_name)\n",
        "env_cfg = registry.get_default_config(env_name)\n",
        "ppo_params = locomotion_params.brax_ppo_config(env_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nibLoRu8anPt"
      },
      "outputs": [],
      "source": [
        "x_data, y_data, y_dataerr = [], [], []\n",
        "times = [datetime.now()]\n",
        "\n",
        "randomizer = registry.get_domain_randomizer(env_name)\n",
        "ppo_training_params = dict(ppo_params)\n",
        "network_factory = ppo_networks.make_ppo_networks\n",
        "if \"network_factory\" in ppo_params:\n",
        "    del ppo_training_params[\"network_factory\"]\n",
        "    network_factory = functools.partial(\n",
        "        ppo_networks.make_ppo_networks, **ppo_params.network_factory\n",
        "    )\n",
        "\n",
        "train_fn = functools.partial(\n",
        "    ppo.train,\n",
        "    **dict(ppo_training_params),\n",
        "    network_factory=network_factory,\n",
        "    randomization_fn=randomizer,\n",
        "    progress_fn=progress\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16dqomv0anPt"
      },
      "outputs": [],
      "source": [
        "make_inference_fn, params, metrics = train_fn(\n",
        "    environment=env,\n",
        "    eval_env=registry.load(env_name, config=env_cfg),\n",
        "    wrap_env_fn=wrapper.wrap_for_brax_training,\n",
        ")\n",
        "print(f\"time to jit: {times[1] - times[0]}\")\n",
        "print(f\"time to train: {times[-1] - times[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sBHDF-JFanPt"
      },
      "outputs": [],
      "source": [
        "# @title Rollout and Render\n",
        "from mujoco_playground._src.gait import draw_joystick_command\n",
        "\n",
        "env = registry.load(env_name)\n",
        "eval_env = registry.load(env_name)\n",
        "jit_reset = jax.jit(eval_env.reset)\n",
        "jit_step = jax.jit(eval_env.step)\n",
        "jit_inference_fn = jax.jit(make_inference_fn(params, deterministic=True))\n",
        "\n",
        "rng = jax.random.PRNGKey(1)\n",
        "\n",
        "rollout = []\n",
        "modify_scene_fns = []\n",
        "\n",
        "x_vel = 1.0  # @param {type: \"number\"}\n",
        "y_vel = 0.0  # @param {type: \"number\"}\n",
        "yaw_vel = 0.0  # @param {type: \"number\"}\n",
        "command = jp.array([x_vel, y_vel, yaw_vel])\n",
        "\n",
        "phase_dt = 2 * jp.pi * eval_env.dt * 1.5\n",
        "phase = jp.array([0, jp.pi])\n",
        "\n",
        "for j in range(1):\n",
        "    print(f\"episode {j}\")\n",
        "    state = jit_reset(rng)\n",
        "    state.info[\"phase_dt\"] = phase_dt\n",
        "    state.info[\"phase\"] = phase\n",
        "    for i in range(env_cfg.episode_length):\n",
        "        act_rng, rng = jax.random.split(rng)\n",
        "        ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
        "        state = jit_step(state, ctrl)\n",
        "        if state.done:\n",
        "            break\n",
        "        state.info[\"command\"] = command\n",
        "        rollout.append(state)\n",
        "\n",
        "        xyz = np.array(state.data.xpos[eval_env.mj_model.body(\"torso\").id])\n",
        "        xyz += np.array([0, 0.0, 0])\n",
        "        x_axis = state.data.xmat[eval_env._torso_body_id, 0]\n",
        "        yaw = -np.arctan2(x_axis[1], x_axis[0])\n",
        "        modify_scene_fns.append(\n",
        "            functools.partial(\n",
        "                draw_joystick_command,\n",
        "                cmd=state.info[\"command\"],\n",
        "                xyz=xyz,\n",
        "                theta=yaw,\n",
        "                scl=np.linalg.norm(state.info[\"command\"]),\n",
        "            )\n",
        "        )\n",
        "\n",
        "render_every = 1\n",
        "fps = 1.0 / eval_env.dt / render_every\n",
        "print(f\"fps: {fps}\")\n",
        "traj = rollout[::render_every]\n",
        "mod_fns = modify_scene_fns[::render_every]\n",
        "\n",
        "scene_option = mujoco.MjvOption()\n",
        "scene_option.geomgroup[2] = True\n",
        "scene_option.geomgroup[3] = False\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_PERTFORCE] = False\n",
        "\n",
        "frames = eval_env.render(\n",
        "    traj,\n",
        "    camera=\"track\",\n",
        "    scene_option=scene_option,\n",
        "    width=640 * 2,\n",
        "    height=480,\n",
        "    modify_scene_fns=mod_fns,\n",
        ")\n",
        "media.show_video(frames, fps=fps, loop=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBtrAqns35sI"
      },
      "source": [
        "🙌 Hasta la vista!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "d3rl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
